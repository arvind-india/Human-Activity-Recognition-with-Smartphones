{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human Activity Recognition with Smartphones\n",
    "\n",
    "https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones\n",
    "\n",
    "Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21st European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>0.322563</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>-0.099583</td>\n",
       "      <td>-0.077319</td>\n",
       "      <td>0.163397</td>\n",
       "      <td>-0.366976</td>\n",
       "      <td>-0.150695</td>\n",
       "      <td>0.081617</td>\n",
       "      <td>-0.354550</td>\n",
       "      <td>0.417399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273368</td>\n",
       "      <td>-0.258125</td>\n",
       "      <td>-0.394798</td>\n",
       "      <td>0.890667</td>\n",
       "      <td>0.550559</td>\n",
       "      <td>-0.672006</td>\n",
       "      <td>0.324277</td>\n",
       "      <td>0.032366</td>\n",
       "      <td>6</td>\n",
       "      <td>WALKING_UPSTAIRS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>0.273518</td>\n",
       "      <td>-0.015485</td>\n",
       "      <td>-0.099208</td>\n",
       "      <td>-0.994748</td>\n",
       "      <td>-0.989076</td>\n",
       "      <td>-0.983350</td>\n",
       "      <td>-0.995697</td>\n",
       "      <td>-0.990311</td>\n",
       "      <td>-0.984223</td>\n",
       "      <td>-0.938327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355408</td>\n",
       "      <td>-0.024263</td>\n",
       "      <td>-0.120520</td>\n",
       "      <td>-0.124300</td>\n",
       "      <td>-0.295777</td>\n",
       "      <td>-0.651881</td>\n",
       "      <td>-0.068769</td>\n",
       "      <td>-0.163118</td>\n",
       "      <td>23</td>\n",
       "      <td>SITTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.348976</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>-0.171332</td>\n",
       "      <td>0.258261</td>\n",
       "      <td>0.066264</td>\n",
       "      <td>-0.193373</td>\n",
       "      <td>0.252908</td>\n",
       "      <td>0.008215</td>\n",
       "      <td>-0.185317</td>\n",
       "      <td>0.455011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916922</td>\n",
       "      <td>-0.184415</td>\n",
       "      <td>0.777533</td>\n",
       "      <td>0.850702</td>\n",
       "      <td>0.827569</td>\n",
       "      <td>-0.753401</td>\n",
       "      <td>0.223178</td>\n",
       "      <td>-0.101939</td>\n",
       "      <td>11</td>\n",
       "      <td>WALKING_DOWNSTAIRS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>0.279097</td>\n",
       "      <td>-0.015661</td>\n",
       "      <td>-0.105164</td>\n",
       "      <td>-0.996365</td>\n",
       "      <td>-0.994423</td>\n",
       "      <td>-0.989382</td>\n",
       "      <td>-0.997014</td>\n",
       "      <td>-0.993650</td>\n",
       "      <td>-0.990283</td>\n",
       "      <td>-0.937397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.812685</td>\n",
       "      <td>-0.075841</td>\n",
       "      <td>-0.225018</td>\n",
       "      <td>-0.351919</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>-0.881652</td>\n",
       "      <td>0.114711</td>\n",
       "      <td>0.102710</td>\n",
       "      <td>16</td>\n",
       "      <td>SITTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>0.449566</td>\n",
       "      <td>-0.025920</td>\n",
       "      <td>-0.091716</td>\n",
       "      <td>0.255458</td>\n",
       "      <td>0.215342</td>\n",
       "      <td>0.248882</td>\n",
       "      <td>0.146518</td>\n",
       "      <td>0.127807</td>\n",
       "      <td>0.178287</td>\n",
       "      <td>0.516075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.636093</td>\n",
       "      <td>-0.801188</td>\n",
       "      <td>0.719251</td>\n",
       "      <td>0.882085</td>\n",
       "      <td>-0.364118</td>\n",
       "      <td>-0.819634</td>\n",
       "      <td>0.157803</td>\n",
       "      <td>0.132478</td>\n",
       "      <td>23</td>\n",
       "      <td>WALKING_DOWNSTAIRS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
       "1151           0.322563           0.001762          -0.099583   \n",
       "4926           0.273518          -0.015485          -0.099208   \n",
       "2018           0.348976           0.008519          -0.171332   \n",
       "2931           0.279097          -0.015661          -0.105164   \n",
       "5023           0.449566          -0.025920          -0.091716   \n",
       "\n",
       "      tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
       "1151         -0.077319          0.163397         -0.366976         -0.150695   \n",
       "4926         -0.994748         -0.989076         -0.983350         -0.995697   \n",
       "2018          0.258261          0.066264         -0.193373          0.252908   \n",
       "2931         -0.996365         -0.994423         -0.989382         -0.997014   \n",
       "5023          0.255458          0.215342          0.248882          0.146518   \n",
       "\n",
       "      tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X  \\\n",
       "1151          0.081617         -0.354550          0.417399   \n",
       "4926         -0.990311         -0.984223         -0.938327   \n",
       "2018          0.008215         -0.185317          0.455011   \n",
       "2931         -0.993650         -0.990283         -0.937397   \n",
       "5023          0.127807          0.178287          0.516075   \n",
       "\n",
       "             ...          fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "1151         ...                                -0.273368   \n",
       "4926         ...                                -0.355408   \n",
       "2018         ...                                -0.916922   \n",
       "2931         ...                                -0.812685   \n",
       "5023         ...                                -0.636093   \n",
       "\n",
       "      angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "1151                    -0.258125                             -0.394798   \n",
       "4926                    -0.024263                             -0.120520   \n",
       "2018                    -0.184415                              0.777533   \n",
       "2931                    -0.075841                             -0.225018   \n",
       "5023                    -0.801188                              0.719251   \n",
       "\n",
       "      angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "1151                          0.890667                              0.550559   \n",
       "4926                         -0.124300                             -0.295777   \n",
       "2018                          0.850702                              0.827569   \n",
       "2931                         -0.351919                              0.015259   \n",
       "5023                          0.882085                             -0.364118   \n",
       "\n",
       "      angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  \\\n",
       "1151             -0.672006              0.324277              0.032366   \n",
       "4926             -0.651881             -0.068769             -0.163118   \n",
       "2018             -0.753401              0.223178             -0.101939   \n",
       "2931             -0.881652              0.114711              0.102710   \n",
       "5023             -0.819634              0.157803              0.132478   \n",
       "\n",
       "      subject            Activity  \n",
       "1151        6    WALKING_UPSTAIRS  \n",
       "4926       23             SITTING  \n",
       "2018       11  WALKING_DOWNSTAIRS  \n",
       "2931       16             SITTING  \n",
       "5023       23  WALKING_DOWNSTAIRS  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 563)\n",
      "(2947, 563)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load up the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)\n",
    "\n",
    "# let's take a gander\n",
    "display(train.head())\n",
    "\n",
    "print train.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data consists of 7352 instances of data with 561 total features\n",
      "Training data includes value counts of\n",
      "LAYING                1407\n",
      "STANDING              1374\n",
      "SITTING               1286\n",
      "WALKING               1226\n",
      "WALKING_UPSTAIRS      1073\n",
      "WALKING_DOWNSTAIRS     986\n",
      "Name: Activity, dtype: int64\n",
      "Testing data consists of 2947 instances of data\n",
      "Testing data includes value counts of\n",
      "LAYING                537\n",
      "STANDING              532\n",
      "WALKING               496\n",
      "SITTING               491\n",
      "WALKING_UPSTAIRS      471\n",
      "WALKING_DOWNSTAIRS    420\n",
      "Name: Activity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Seperate subject information\n",
    "subject_training_data = train['subject']\n",
    "subject_testing_data = test['subject']\n",
    "\n",
    "# Seperate labels\n",
    "training_labels = train['Activity']\n",
    "testing_labels = test['Activity']\n",
    "\n",
    "# Drop labels and subject info from data\n",
    "train = train.drop(['subject', 'Activity'], axis=1)\n",
    "test = test.drop(['subject', 'Activity'], axis=1)\n",
    "\n",
    "# Print some information about our data\n",
    "print \"Training data consists of {} instances of data with {} total features\".format(train.shape[0], train.shape[1])\n",
    "print \"Training data includes value counts of\\n\", training_labels.value_counts()\n",
    "print \"Testing data consists of {} instances of data\".format(test.shape[0])\n",
    "print \"Testing data includes value counts of\\n\", testing_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "#scaled_trainingdata = scaler.fit_transform(train)\n",
    "\n",
    "# Encode our categorical labels into numerical target labels\n",
    "le = LabelEncoder()\n",
    "le = le.fit([\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LAYING\"])\n",
    "enc_training_labels = le.transform(training_labels)\n",
    "enc_testing_labels = le.transform(testing_labels)\n",
    "\n",
    "\n",
    "#tsne = TSNE(init = 'pca')\n",
    "#tsne_vis = tsne.fit_transform(scaled_trainingdata)\n",
    "#plt.scatter(tsne_vis[:,0], tsne_vis[:,1], c=encodedlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree performances: [ 0.93393148  0.93637847  0.93913399], \n",
      "Average: 0.936481312663\n",
      "Random Forest performances: [ 0.96655791  0.9677814   0.97222222], \n",
      "Average: 0.968853845689\n",
      "Extra Trees performances: [ 0.96615008  0.9677814   0.96813725], \n",
      "Average: 0.967356246468\n",
      "K Neighbors performances: [ 0.96615008  0.95880914  0.9620098 ], \n",
      "Average: 0.962323006962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "#Let's try out some out-of-the-box classifiers and see how they perform\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "xt = ExtraTreesClassifier()\n",
    "kn = KNeighborsClassifier()\n",
    "\n",
    "def evaluateclf(clf):\n",
    "    scores = cross_val_score(clf, train, enc_training_labels)\n",
    "    avg = scores.mean()\n",
    "    return \"performances: {}, \\nAverage: {}\".format(scores, avg)\n",
    "\n",
    "print \"Decision Tree {}\".format(evaluateclf(dt))\n",
    "\n",
    "print \"Random Forest {}\".format(evaluateclf(rf))\n",
    "\n",
    "print \"Extra Trees {}\".format(evaluateclf(xt))\n",
    "\n",
    "print \"K Neighbors {}\".format(evaluateclf(kn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=4)]: Done  90 out of  90 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Extremely Random Trees classifier looks promising, let's fine tune some hyper-parameters and see how much we can improve\n",
    "parameters = {'n_estimators': np.arange(20,200,20), 'min_samples_split': np.arange(2,10,2)}\n",
    "\n",
    "randgrid = RandomizedSearchCV(xt, parameters, n_iter = 30, n_jobs = 4, verbose = 3)\n",
    "\n",
    "randgrid = randgrid.fit(train, enc_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=180, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "0.985446137106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunny\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score for extra random trees is 0.9423\n"
     ]
    }
   ],
   "source": [
    "print randgrid.best_estimator_\n",
    "print randgrid.best_score_\n",
    "\n",
    "# We'll train the model and hyper-parameters which produced the best 3-fold cross-validation score\n",
    "xt = randgrid.best_estimator_\n",
    "xt.fit_transform(train, enc_training_labels)\n",
    "\n",
    "# Check the performance of the tuned and trained model on the testing set\n",
    "print \"Testing score for extra random trees is {:.4f}\".format(xt.score(test, enc_testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Now let's experiment with a neural network to classify this data and see if we can improve our accuracy even further\n",
    "# First we need to encode our targets as one-hot label vectors\n",
    "oh_training_labels = to_categorical(enc_training_labels)\n",
    "oh_testing_labels = to_categorical(enc_testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6249 samples, validate on 1103 samples\n",
      "Epoch 1/120\n",
      "0s - loss: 1.2367 - acc: 0.4673 - val_loss: 0.6198 - val_acc: 0.5956\n",
      "Epoch 2/120\n",
      "0s - loss: 0.9474 - acc: 0.6484 - val_loss: 0.4676 - val_acc: 0.7561\n",
      "Epoch 3/120\n",
      "0s - loss: 0.8080 - acc: 0.7150 - val_loss: 0.3966 - val_acc: 0.7860\n",
      "Epoch 4/120\n",
      "0s - loss: 0.7506 - acc: 0.7337 - val_loss: 0.3987 - val_acc: 0.7815\n",
      "Epoch 5/120\n",
      "0s - loss: 0.6484 - acc: 0.7702 - val_loss: 0.3127 - val_acc: 0.8296\n",
      "Epoch 6/120\n",
      "0s - loss: 0.6041 - acc: 0.7796 - val_loss: 0.3057 - val_acc: 0.8214\n",
      "Epoch 7/120\n",
      "0s - loss: 0.5788 - acc: 0.7969 - val_loss: 0.2723 - val_acc: 0.8867\n",
      "Epoch 8/120\n",
      "0s - loss: 0.5156 - acc: 0.8120 - val_loss: 0.3101 - val_acc: 0.8286\n",
      "Epoch 9/120\n",
      "0s - loss: 0.4906 - acc: 0.8321 - val_loss: 0.2759 - val_acc: 0.8685\n",
      "Epoch 10/120\n",
      "0s - loss: 0.4750 - acc: 0.8361 - val_loss: 0.2486 - val_acc: 0.8676\n",
      "Epoch 11/120\n",
      "0s - loss: 0.4247 - acc: 0.8605 - val_loss: 0.2195 - val_acc: 0.9012\n",
      "Epoch 12/120\n",
      "0s - loss: 0.3926 - acc: 0.8817 - val_loss: 0.2420 - val_acc: 0.8812\n",
      "Epoch 13/120\n",
      "0s - loss: 0.3764 - acc: 0.8934 - val_loss: 0.1705 - val_acc: 0.9220\n",
      "Epoch 14/120\n",
      "0s - loss: 0.3638 - acc: 0.8987 - val_loss: 0.1601 - val_acc: 0.9320\n",
      "Epoch 15/120\n",
      "0s - loss: 0.3380 - acc: 0.9136 - val_loss: 0.1586 - val_acc: 0.9320\n",
      "Epoch 16/120\n",
      "0s - loss: 0.3146 - acc: 0.9168 - val_loss: 0.1638 - val_acc: 0.9338\n",
      "Epoch 17/120\n",
      "0s - loss: 0.3308 - acc: 0.9117 - val_loss: 0.1784 - val_acc: 0.9320\n",
      "Epoch 18/120\n",
      "0s - loss: 0.3143 - acc: 0.9185 - val_loss: 0.1432 - val_acc: 0.9411\n",
      "Epoch 19/120\n",
      "0s - loss: 0.2983 - acc: 0.9240 - val_loss: 0.1486 - val_acc: 0.9447\n",
      "Epoch 20/120\n",
      "0s - loss: 0.2907 - acc: 0.9264 - val_loss: 0.1809 - val_acc: 0.9311\n",
      "Epoch 21/120\n",
      "0s - loss: 0.2709 - acc: 0.9349 - val_loss: 0.1343 - val_acc: 0.9429\n",
      "Epoch 22/120\n",
      "0s - loss: 0.2719 - acc: 0.9312 - val_loss: 0.1326 - val_acc: 0.9474\n",
      "Epoch 23/120\n",
      "0s - loss: 0.2661 - acc: 0.9301 - val_loss: 0.1325 - val_acc: 0.9438\n",
      "Epoch 24/120\n",
      "0s - loss: 0.2701 - acc: 0.9318 - val_loss: 0.1149 - val_acc: 0.9538\n",
      "Epoch 25/120\n",
      "0s - loss: 0.2534 - acc: 0.9374 - val_loss: 0.1373 - val_acc: 0.9411\n",
      "Epoch 26/120\n",
      "0s - loss: 0.2413 - acc: 0.9429 - val_loss: 0.1038 - val_acc: 0.9610\n",
      "Epoch 27/120\n",
      "0s - loss: 0.2337 - acc: 0.9470 - val_loss: 0.1288 - val_acc: 0.9456\n",
      "Epoch 28/120\n",
      "0s - loss: 0.2428 - acc: 0.9418 - val_loss: 0.1087 - val_acc: 0.9547\n",
      "Epoch 29/120\n",
      "0s - loss: 0.2437 - acc: 0.9422 - val_loss: 0.1556 - val_acc: 0.9356\n",
      "Epoch 30/120\n",
      "0s - loss: 0.2233 - acc: 0.9494 - val_loss: 0.1149 - val_acc: 0.9492\n",
      "Epoch 31/120\n",
      "0s - loss: 0.2110 - acc: 0.9528 - val_loss: 0.1183 - val_acc: 0.9583\n",
      "Epoch 32/120\n",
      "0s - loss: 0.2020 - acc: 0.9573 - val_loss: 0.0914 - val_acc: 0.9637\n",
      "Epoch 33/120\n",
      "0s - loss: 0.2029 - acc: 0.9554 - val_loss: 0.0945 - val_acc: 0.9646\n",
      "Epoch 34/120\n",
      "0s - loss: 0.1971 - acc: 0.9574 - val_loss: 0.0790 - val_acc: 0.9683\n",
      "Epoch 35/120\n",
      "0s - loss: 0.1898 - acc: 0.9610 - val_loss: 0.0993 - val_acc: 0.9610\n",
      "Epoch 36/120\n",
      "0s - loss: 0.1812 - acc: 0.9654 - val_loss: 0.1255 - val_acc: 0.9538\n",
      "Epoch 37/120\n",
      "0s - loss: 0.1835 - acc: 0.9632 - val_loss: 0.0829 - val_acc: 0.9692\n",
      "Epoch 38/120\n",
      "0s - loss: 0.1756 - acc: 0.9642 - val_loss: 0.0836 - val_acc: 0.9683\n",
      "Epoch 39/120\n",
      "0s - loss: 0.1707 - acc: 0.9683 - val_loss: 0.1224 - val_acc: 0.9474\n",
      "Epoch 40/120\n",
      "0s - loss: 0.1822 - acc: 0.9619 - val_loss: 0.0782 - val_acc: 0.9692\n",
      "Epoch 41/120\n",
      "0s - loss: 0.1857 - acc: 0.9589 - val_loss: 0.0920 - val_acc: 0.9665\n",
      "Epoch 42/120\n",
      "0s - loss: 0.1704 - acc: 0.9662 - val_loss: 0.0804 - val_acc: 0.9710\n",
      "Epoch 43/120\n",
      "0s - loss: 0.1757 - acc: 0.9606 - val_loss: 0.0845 - val_acc: 0.9701\n",
      "Epoch 44/120\n",
      "0s - loss: 0.1642 - acc: 0.9682 - val_loss: 0.1087 - val_acc: 0.9610\n",
      "Epoch 45/120\n",
      "0s - loss: 0.1733 - acc: 0.9642 - val_loss: 0.0879 - val_acc: 0.9665\n",
      "Epoch 46/120\n",
      "0s - loss: 0.1588 - acc: 0.9691 - val_loss: 0.0811 - val_acc: 0.9674\n",
      "Epoch 47/120\n",
      "0s - loss: 0.1576 - acc: 0.9696 - val_loss: 0.0933 - val_acc: 0.9637\n",
      "Epoch 48/120\n",
      "0s - loss: 0.1523 - acc: 0.9710 - val_loss: 0.0795 - val_acc: 0.9710\n",
      "Epoch 49/120\n",
      "0s - loss: 0.1562 - acc: 0.9670 - val_loss: 0.0782 - val_acc: 0.9692\n",
      "Epoch 50/120\n",
      "0s - loss: 0.1469 - acc: 0.9723 - val_loss: 0.0702 - val_acc: 0.9719\n",
      "Epoch 51/120\n",
      "0s - loss: 0.1591 - acc: 0.9664 - val_loss: 0.0701 - val_acc: 0.9701\n",
      "Epoch 52/120\n",
      "0s - loss: 0.1528 - acc: 0.9701 - val_loss: 0.0715 - val_acc: 0.9719\n",
      "Epoch 53/120\n",
      "0s - loss: 0.1600 - acc: 0.9664 - val_loss: 0.0812 - val_acc: 0.9674\n",
      "Epoch 54/120\n",
      "0s - loss: 0.1419 - acc: 0.9733 - val_loss: 0.0727 - val_acc: 0.9719\n",
      "Epoch 55/120\n",
      "0s - loss: 0.1495 - acc: 0.9702 - val_loss: 0.0979 - val_acc: 0.9637\n",
      "Epoch 56/120\n",
      "0s - loss: 0.1476 - acc: 0.9715 - val_loss: 0.0906 - val_acc: 0.9683\n",
      "Epoch 57/120\n",
      "0s - loss: 0.1455 - acc: 0.9714 - val_loss: 0.0813 - val_acc: 0.9674\n",
      "Epoch 58/120\n",
      "0s - loss: 0.1386 - acc: 0.9760 - val_loss: 0.0764 - val_acc: 0.9710\n",
      "Epoch 59/120\n",
      "0s - loss: 0.1367 - acc: 0.9762 - val_loss: 0.0930 - val_acc: 0.9701\n",
      "Epoch 60/120\n",
      "0s - loss: 0.1362 - acc: 0.9741 - val_loss: 0.0828 - val_acc: 0.9674\n",
      "Epoch 61/120\n",
      "0s - loss: 0.1453 - acc: 0.9691 - val_loss: 0.0677 - val_acc: 0.9755\n",
      "Epoch 62/120\n",
      "0s - loss: 0.1354 - acc: 0.9752 - val_loss: 0.0842 - val_acc: 0.9674\n",
      "Epoch 63/120\n",
      "0s - loss: 0.1298 - acc: 0.9770 - val_loss: 0.0727 - val_acc: 0.9746\n",
      "Epoch 64/120\n",
      "0s - loss: 0.1490 - acc: 0.9672 - val_loss: 0.0770 - val_acc: 0.9719\n",
      "Epoch 65/120\n",
      "0s - loss: 0.1279 - acc: 0.9784 - val_loss: 0.0900 - val_acc: 0.9710\n",
      "Epoch 66/120\n",
      "0s - loss: 0.1363 - acc: 0.9752 - val_loss: 0.0798 - val_acc: 0.9728\n",
      "Epoch 67/120\n",
      "0s - loss: 0.1297 - acc: 0.9760 - val_loss: 0.0663 - val_acc: 0.9737\n",
      "Epoch 68/120\n",
      "0s - loss: 0.1229 - acc: 0.9778 - val_loss: 0.0654 - val_acc: 0.9701\n",
      "Epoch 69/120\n",
      "0s - loss: 0.1318 - acc: 0.9730 - val_loss: 0.0607 - val_acc: 0.9764\n",
      "Epoch 70/120\n",
      "0s - loss: 0.1285 - acc: 0.9750 - val_loss: 0.0634 - val_acc: 0.9728\n",
      "Epoch 71/120\n",
      "0s - loss: 0.1294 - acc: 0.9768 - val_loss: 0.0655 - val_acc: 0.9755\n",
      "Epoch 72/120\n",
      "0s - loss: 0.1312 - acc: 0.9741 - val_loss: 0.0655 - val_acc: 0.9755\n",
      "Epoch 73/120\n",
      "0s - loss: 0.1292 - acc: 0.9744 - val_loss: 0.1740 - val_acc: 0.9329\n",
      "Epoch 74/120\n",
      "0s - loss: 0.1413 - acc: 0.9709 - val_loss: 0.0619 - val_acc: 0.9737\n",
      "Epoch 75/120\n",
      "0s - loss: 0.1249 - acc: 0.9763 - val_loss: 0.0940 - val_acc: 0.9701\n",
      "Epoch 76/120\n",
      "0s - loss: 0.1291 - acc: 0.9744 - val_loss: 0.0870 - val_acc: 0.9728\n",
      "Epoch 77/120\n",
      "0s - loss: 0.1214 - acc: 0.9786 - val_loss: 0.0750 - val_acc: 0.9737\n",
      "Epoch 78/120\n",
      "0s - loss: 0.1207 - acc: 0.9794 - val_loss: 0.0731 - val_acc: 0.9719\n",
      "Epoch 79/120\n",
      "0s - loss: 0.1212 - acc: 0.9768 - val_loss: 0.0659 - val_acc: 0.9719\n",
      "Epoch 80/120\n",
      "0s - loss: 0.1193 - acc: 0.9787 - val_loss: 0.0588 - val_acc: 0.9746\n",
      "Epoch 81/120\n",
      "0s - loss: 0.1294 - acc: 0.9723 - val_loss: 0.0802 - val_acc: 0.9719\n",
      "Epoch 82/120\n",
      "0s - loss: 0.1242 - acc: 0.9758 - val_loss: 0.0614 - val_acc: 0.9755\n",
      "Epoch 83/120\n",
      "0s - loss: 0.1190 - acc: 0.9782 - val_loss: 0.0880 - val_acc: 0.9692\n",
      "Epoch 84/120\n",
      "0s - loss: 0.1132 - acc: 0.9787 - val_loss: 0.1108 - val_acc: 0.9565\n",
      "Epoch 85/120\n",
      "0s - loss: 0.1216 - acc: 0.9782 - val_loss: 0.0619 - val_acc: 0.9737\n",
      "Epoch 86/120\n",
      "0s - loss: 0.1220 - acc: 0.9776 - val_loss: 0.0676 - val_acc: 0.9764\n",
      "Epoch 87/120\n",
      "0s - loss: 0.1185 - acc: 0.9786 - val_loss: 0.0590 - val_acc: 0.9755\n",
      "Epoch 88/120\n",
      "0s - loss: 0.1123 - acc: 0.9790 - val_loss: 0.0811 - val_acc: 0.9692\n",
      "Epoch 89/120\n",
      "0s - loss: 0.1257 - acc: 0.9742 - val_loss: 0.0646 - val_acc: 0.9791\n",
      "Epoch 90/120\n",
      "0s - loss: 0.1112 - acc: 0.9805 - val_loss: 0.0844 - val_acc: 0.9710\n",
      "Epoch 91/120\n",
      "0s - loss: 0.1157 - acc: 0.9781 - val_loss: 0.0592 - val_acc: 0.9746\n",
      "Epoch 92/120\n",
      "0s - loss: 0.1106 - acc: 0.9826 - val_loss: 0.0635 - val_acc: 0.9755\n",
      "Epoch 93/120\n",
      "0s - loss: 0.1149 - acc: 0.9778 - val_loss: 0.0666 - val_acc: 0.9746\n",
      "Epoch 94/120\n",
      "0s - loss: 0.1106 - acc: 0.9816 - val_loss: 0.0615 - val_acc: 0.9764\n",
      "Epoch 95/120\n",
      "0s - loss: 0.1207 - acc: 0.9776 - val_loss: 0.0611 - val_acc: 0.9764\n",
      "Epoch 96/120\n",
      "0s - loss: 0.1159 - acc: 0.9779 - val_loss: 0.0630 - val_acc: 0.9819\n",
      "Epoch 97/120\n",
      "0s - loss: 0.1064 - acc: 0.9830 - val_loss: 0.0717 - val_acc: 0.9755\n",
      "Epoch 98/120\n",
      "0s - loss: 0.1200 - acc: 0.9757 - val_loss: 0.1001 - val_acc: 0.9683\n",
      "Epoch 99/120\n",
      "0s - loss: 0.1094 - acc: 0.9818 - val_loss: 0.0618 - val_acc: 0.9773\n",
      "Epoch 100/120\n",
      "0s - loss: 0.1145 - acc: 0.9781 - val_loss: 0.0603 - val_acc: 0.9755\n",
      "Epoch 101/120\n",
      "0s - loss: 0.1066 - acc: 0.9805 - val_loss: 0.1026 - val_acc: 0.9610\n",
      "Epoch 102/120\n",
      "0s - loss: 0.1129 - acc: 0.9770 - val_loss: 0.0923 - val_acc: 0.9683\n",
      "Epoch 103/120\n",
      "0s - loss: 0.1180 - acc: 0.9771 - val_loss: 0.0600 - val_acc: 0.9746\n",
      "Epoch 104/120\n",
      "0s - loss: 0.1108 - acc: 0.9806 - val_loss: 0.0990 - val_acc: 0.9592\n",
      "Epoch 105/120\n",
      "0s - loss: 0.1113 - acc: 0.9779 - val_loss: 0.0618 - val_acc: 0.9819\n",
      "Epoch 106/120\n",
      "0s - loss: 0.1089 - acc: 0.9789 - val_loss: 0.0567 - val_acc: 0.9773\n",
      "Epoch 107/120\n",
      "0s - loss: 0.1104 - acc: 0.9798 - val_loss: 0.0558 - val_acc: 0.9782\n",
      "Epoch 108/120\n",
      "0s - loss: 0.1009 - acc: 0.9838 - val_loss: 0.0612 - val_acc: 0.9819\n",
      "Epoch 109/120\n",
      "0s - loss: 0.1059 - acc: 0.9819 - val_loss: 0.0590 - val_acc: 0.9782\n",
      "Epoch 110/120\n",
      "0s - loss: 0.1144 - acc: 0.9758 - val_loss: 0.0567 - val_acc: 0.9764\n",
      "Epoch 111/120\n",
      "0s - loss: 0.1011 - acc: 0.9826 - val_loss: 0.1241 - val_acc: 0.9529\n",
      "Epoch 112/120\n",
      "0s - loss: 0.1093 - acc: 0.9795 - val_loss: 0.0613 - val_acc: 0.9810\n",
      "Epoch 113/120\n",
      "0s - loss: 0.1088 - acc: 0.9798 - val_loss: 0.0558 - val_acc: 0.9773\n",
      "Epoch 114/120\n",
      "0s - loss: 0.1056 - acc: 0.9819 - val_loss: 0.0620 - val_acc: 0.9819\n",
      "Epoch 115/120\n",
      "0s - loss: 0.1043 - acc: 0.9822 - val_loss: 0.0561 - val_acc: 0.9755\n",
      "Epoch 116/120\n",
      "0s - loss: 0.0963 - acc: 0.9838 - val_loss: 0.0702 - val_acc: 0.9791\n",
      "Epoch 117/120\n",
      "0s - loss: 0.1001 - acc: 0.9830 - val_loss: 0.0578 - val_acc: 0.9773\n",
      "Epoch 118/120\n",
      "0s - loss: 0.0980 - acc: 0.9845 - val_loss: 0.0597 - val_acc: 0.9755\n",
      "Epoch 119/120\n",
      "0s - loss: 0.1038 - acc: 0.9813 - val_loss: 0.0716 - val_acc: 0.9737\n",
      "Epoch 120/120\n",
      "0s - loss: 0.1022 - acc: 0.9813 - val_loss: 0.0576 - val_acc: 0.9773\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Build a network for this classification task\n",
    "model = Sequential()\n",
    "model.add(Dense(96, input_dim = train.shape[1], activation = 'tanh', init = 'lecun_uniform', W_regularizer=l2(.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(30, activation = 'tanh', init = 'lecun_uniform', W_regularizer=l2(.001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(24, activation = 'tanh', init = 'lecun_uniform', W_regularizer=l2(.0005)))\n",
    "model.add(Dense(output_dim = 6, activation = 'softmax'))\n",
    "\n",
    "sgd = SGD(lr = .13, momentum = .9, decay = 4e-3)\n",
    "#early_stopping = EarlyStopping(monitor = 'val_acc', min_delta = .002, patience = 7, mode = 'auto')\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(train.values, oh_training_labels, nb_epoch = 120, batch_size = 50, verbose = 2,\n",
    "          validation_split = .15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our neural network achieves an accuracy of 0.962673905667 on the test set\n"
     ]
    }
   ],
   "source": [
    "#implement early stopping?\n",
    "nn_test_score = model.evaluate(test.values, oh_testing_labels, verbose=2)\n",
    "print \"Our neural network achieves an accuracy of {} on the test set\".format(nn_test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our neural network gives an improvement in accuracy of ~2% on the testing data ! :^)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9Q8LGd6RRhhNnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20422128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('categorical cross entropy loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training loss', 'validation loss'], loc = 'upper right' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
